# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L32
seed: 42

n_timesteps: !!float 20e6
policy: 'MlpPolicy'
n_steps: 32 #4096 # The number of steps to run for each environment per update 
                    # (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) 
batch_size: 256 # Minibatch size
gae_lambda: 0.95
gamma: 0.99
n_epochs: 5 #20 # Number of epoch when optimizing the surrogate loss
ent_coef: 0.02
learning_rate: !!float 3e-4
decaying_learning_rate: !!bool true
clip_range: !!float 0.2
policy_kwargs: "dict(
                  activation_fn=nn.ReLU,
                  net_arch=[192,64],
                  squash_output=False,
                )"
vf_coef: 1.0
max_grad_norm: 1.0
device: "cuda:0"

# When using on-policy methods like PPO, you collect a large batch of experience (a rollout) from your environments.
# The rollout is given by the number of steps per environment (n_steps) multiplied by the number of environments (n_envs).
# This rollout is then divided into mini-batches, and each mini-batch is used to perform a gradient descent update.
# The batch size determines the size of these mini-batches.
