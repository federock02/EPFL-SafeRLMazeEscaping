{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e847fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from skrl.agents.torch.ppo import PPO\n",
    "from skrl.models.torch import GaussianMixin, Model\n",
    "from skrl.resources.preprocessors.torch import RunningStandardScaler\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "from skrl.envs.torch import wrap_env\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42b254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space: int = 12\n",
    "action_space: int = 2\n",
    "action_scale: float = 1.0\n",
    "network_path = \"/home/federico/isaaclab/IsaacLab/logs/skrl/jetbot_direct_ppo/2025-04-05_17-46-30_ppo_torch/checkpoints/best_agent.pt\"\n",
    "cfg_path = \"/home/federico/isaaclab/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/direct/jetbot/agents/skrl_ppo_lagrangian_cfg.yaml\"\n",
    "with open(cfg_path, 'r') as f:\n",
    "    cfg_dict = yaml.safe_load(f)\n",
    "    models_cfg = cfg_dict['models']\n",
    "    agent_cfg = cfg_dict['agent']\n",
    "    memory_cfg = cfg_dict['memory']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy_network = models_cfg['policy']['network'][0]\n",
    "clip_actions = models_cfg['policy']['clip_actions']\n",
    "clip_log_std = models_cfg['policy']['clip_log_std']\n",
    "min_log_std = models_cfg['policy']['min_log_std']\n",
    "max_log_std = models_cfg['policy']['max_log_std']\n",
    "initial_log_std = models_cfg['policy']['initial_log_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b695eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class Policy(GaussianMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, clip_actions=False,\n",
    "                 clip_log_std=True, min_log_std=-20, max_log_std=2):\n",
    "        Model.__init__(self, observation_space, action_space, device)\n",
    "        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std)\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        policy_layers = []\n",
    "        input_dim = observation_space\n",
    "        for layer_size in policy_network['layers']:\n",
    "            policy_layers.append(nn.Linear(input_dim, layer_size))\n",
    "            if policy_network['activations'] == 'relu' or policy_network['activations'] == 'Relu' or policy_network['activations'] == 'ReLU' or policy_network['activations'] == 'RELU':\n",
    "                policy_layers.append(nn.ReLU())\n",
    "            else:\n",
    "                print(\"ERROR: Unsupported activation function. Please define it.\")\n",
    "            # policy_layers.append(getattr(nn, policy_network_config['activations'])())\n",
    "            input_dim = layer_size\n",
    "        policy_layers.append(nn.Linear(input_dim, action_space)) # Output layer for mean\n",
    "\n",
    "        self.net = nn.Sequential(*policy_layers)\n",
    "        self.log_std_parameters = nn.Parameter(torch.zeros(action_space))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        return self.net(inputs[\"states\"]), self.log_std_parameters, {}\n",
    "\n",
    "# Instantiate the model\n",
    "policy = Policy(observation_space, action_space, device, clip_actions=clip_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys available in the checkpoint's policy state_dict: odict_keys(['log_std_parameter', 'net_container.0.weight', 'net_container.0.bias', 'net_container.2.weight', 'net_container.2.bias', 'policy_layer.weight', 'policy_layer.bias', 'value_layer.weight', 'value_layer.bias'])\n",
      "Keys expected by the target model's state_dict: odict_keys(['log_std_parameters', 'net.0.weight', 'net.0.bias', 'net.2.weight', 'net.2.bias', 'net.4.weight', 'net.4.bias'])\n",
      "Ignoring value layer key: value_layer.weight\n",
      "Ignoring value layer key: value_layer.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20202/3404460741.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(network_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=192, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the weights from the checkpoint\n",
    "checkpoint = torch.load(network_path, map_location=device)\n",
    "\n",
    "if 'policy' in checkpoint:\n",
    "    checkpoint_policy_state_dict = checkpoint['policy']\n",
    "elif 'model' in checkpoint and 'policy' in checkpoint['model']:\n",
    "     # Sometimes it's nested like in RL-games checkpoints\n",
    "    checkpoint_policy_state_dict = checkpoint['model']['policy']\n",
    "else:\n",
    "    raise KeyError(\"Could not find 'policy' state_dict in the checkpoint.\")\n",
    "\n",
    "print(\"Keys available in the checkpoint's policy state_dict:\", checkpoint_policy_state_dict.keys())\n",
    "print(\"Keys expected by the target model's state_dict:\", policy.state_dict().keys())\n",
    "\n",
    "new_state_dict = {}\n",
    "key_mapping = {\n",
    "    \"log_std_parameter\": \"log_std_parameters\",\n",
    "    \"net_container.0.weight\": \"net.0.weight\",\n",
    "    \"net_container.0.bias\": \"net.0.bias\",\n",
    "    \"net_container.2.weight\": \"net.2.weight\",\n",
    "    \"net_container.2.bias\": \"net.2.bias\",\n",
    "    # Assuming 'policy_layer' in the checkpoint corresponds to the *final* linear layer\n",
    "    # in your Sequential 'net'. Based on the sequential structure [Linear, ReLU, Linear, ReLU, Linear]\n",
    "    # the indices are 0, 1, 2, 3, 4. The last linear layer is at index 4.\n",
    "    \"policy_layer.weight\": \"net.4.weight\",\n",
    "    \"policy_layer.bias\": \"net.4.bias\"\n",
    "}\n",
    "\n",
    "for k, v in checkpoint_policy_state_dict.items():\n",
    "    # Check if this checkpoint key is in our mapping\n",
    "    if k in key_mapping:\n",
    "        target_key = key_mapping[k]\n",
    "        # Check if the target key actually exists in the current model\n",
    "        if target_key in policy.state_dict():\n",
    "             new_state_dict[target_key] = v\n",
    "             # print(f\"Mapped '{k}' to '{target_key}'\") # Optional: uncomment for debugging\n",
    "        else:\n",
    "             print(f\"Warning: Mapped target key '{target_key}' for checkpoint key '{k}' not found in the target model. Skipping.\")\n",
    "    # Optionally, handle keys you explicitly want to ignore (like value_layer)\n",
    "    elif k.startswith(\"value_layer.\"):\n",
    "         print(f\"Ignoring value layer key: {k}\")\n",
    "         pass\n",
    "    # Optional: Warn about any keys in the checkpoint['policy'] that weren't mapped or ignored\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint key '{k}' is not included in the explicit mapping or ignore list. Skipping.\")\n",
    "\n",
    "\n",
    "# load the weights into the model\n",
    "policy.load_state_dict(new_state_dict, strict=False)\n",
    "policy.to(device)\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f59ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [13.799861 12.745668]\n"
     ]
    }
   ],
   "source": [
    "observation = torch.rand(observation_space, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get distribution parameters (mean, log_std) from the policy\n",
    "    mean, log_std_parameters, _ = policy.compute({\"states\": observation}, role=\"policy\")\n",
    "\n",
    "print(\"Action:\", mean.squeeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8726f96a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate the agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# models dict\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# memory instance, or None if not required\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# configuration dict (preprocessors, learning rate schedulers, etc.)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m agent\u001b[38;5;241m.\u001b[39mload(network_path)  \u001b[38;5;66;03m# load the model from the specified path\u001b[39;00m\n",
      "File \u001b[0;32m~/isaacsim-env/lib/python3.10/site-packages/skrl/agents/torch/ppo/ppo.py:106\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, models, memory, observation_space, action_space, device, cfg)\u001b[0m\n\u001b[1;32m    104\u001b[0m _cfg \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(PPO_DEFAULT_CONFIG)\n\u001b[1;32m    105\u001b[0m _cfg\u001b[38;5;241m.\u001b[39mupdate(cfg \u001b[38;5;28;01mif\u001b[39;00m cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# models\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/isaacsim-env/lib/python3.10/site-packages/skrl/agents/torch/base.py:64\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, models, memory, observation_space, action_space, device, cfg)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracking_data \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "agent = PPO(models=models_cfg,  # models dict\n",
    "            memory=memory_cfg,  # memory instance, or None if not required\n",
    "            cfg=agent_cfg,  # configuration dict (preprocessors, learning rate schedulers, etc.)\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "            device=device)\n",
    "\n",
    "agent.load(network_path)  # load the model from the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d93374c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m observation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(observation_space)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# example observation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mact(observation, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# get the action from the agent\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, action)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "observation = torch.randn(observation_space).to(device)  # example observation\n",
    "action = agent.act(observation, 0, 1)  # get the action from the agent\n",
    "print(\"Action:\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bc48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkController:\n",
    "    def __init__(self):\n",
    "        self.observation_space: int = 12\n",
    "        self.action_space: int = 2\n",
    "        self.action_scale: float = 1.0\n",
    "        network_path = \"/home/federico/isaaclab/IsaacLab/logs/skrl/jetbot_direct_ppo/2025-04-05_17-46-30_ppo_torch/final_model.zip\"\n",
    "        cfg_path = \"/home/federico/isaaclab/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/direct/jetbot/agents/skrl_ppo_lagrangian_cfg.yaml\"\n",
    "        with open(cfg_path, 'r') as f:\n",
    "            cfg_dict = yaml.safe_load(f)\n",
    "            models_cfg = cfg_dict['models']\n",
    "            agent_cfg = cfg_dict['agent']\n",
    "            memory_cfg = cfg_dict['memory']\n",
    "\n",
    "        # Manually instantiate the policy network\n",
    "        policy_network_config = models_cfg['policy']['network'][0] # Access the first dictionary in the list\n",
    "        policy_layers = []\n",
    "        input_dim = self.observation_space\n",
    "        for layer_size in policy_network_config['layers']:\n",
    "            policy_layers.append(nn.Linear(input_dim, layer_size))\n",
    "            if policy_network_config['activations'] == 'relu' or policy_network_config['activations'] == 'Relu' or policy_network_config['activations'] == 'ReLU' or policy_network_config['activations'] == 'RELU':\n",
    "                policy_layers.append(nn.ReLU())\n",
    "            else:\n",
    "                print(\"ERROR: Unsupported activation function. Please define it.\")\n",
    "            # policy_layers.append(getattr(nn, policy_network_config['activations'])())\n",
    "            input_dim = layer_size\n",
    "        policy_layers.append(nn.Linear(input_dim, self.action_space)) # Output layer for mean\n",
    "\n",
    "        policy_class = models_cfg['policy']['class']\n",
    "        clip_actions = models_cfg['policy']['clip_actions']\n",
    "        clip_log_std = models_cfg['policy']['clip_log_std']\n",
    "        min_log_std = models_cfg['policy']['min_log_std']\n",
    "        max_log_std = models_cfg['policy']['max_log_std']\n",
    "        initial_log_std = models_cfg['policy']['initial_log_std']\n",
    "\n",
    "        class PolicyNetwork(GaussianMixin, Model):\n",
    "            def __init__(self, observation_space, action_space, clip_actions=True, clip_log_std=False,\n",
    "                         min_log_std=-20.0, max_log_std=2.0, initial_log_std=0.0, device='cpu'):\n",
    "                GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std)\n",
    "                Model.__init__(self, observation_space, action_space, device)\n",
    "                self.net = nn.Sequential(*policy_layers)\n",
    "                self.mean_layer = nn.Linear(policy_network_config['layers'][-1] if policy_network_config['layers'] else observation_space, action_space)\n",
    "                self.log_std_parameter = nn.Parameter(torch.full((action_space,), initial_log_std))\n",
    "                self.device = device\n",
    "                self.to(self.device)\n",
    "\n",
    "            def forward(self, states, taken_actions=None, inference=False):\n",
    "                output = self.net(states)\n",
    "                mean = self.mean_layer(output)\n",
    "                log_std = self.log_std_parameter.expand_as(mean)\n",
    "                distribution = torch.distributions.Normal(mean, log_std.exp())\n",
    "                return mean, log_std\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        policy = PolicyNetwork(self.observation_space, self.action_space, clip_actions, clip_log_std,\n",
    "                               min_log_std, max_log_std, initial_log_std, device)\n",
    "\n",
    "        # Instantiate the PPO agent with the manually created policy\n",
    "        self.agent = PPO(models={\"policy\": policy},\n",
    "                    memory=memory_cfg,\n",
    "                    cfg=agent_cfg,\n",
    "                    observation_space=self.observation_space,\n",
    "                    action_space=self.action_space,\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        )\n",
    "\n",
    "        # Load the pre-trained weights\n",
    "        self.agent.load(network_path)\n",
    "        self.agent.set_mode('eval')\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        \"\"\"\n",
    "        Get the action from the agent given an observation.\n",
    "        :param observation: The observation from the environment.\n",
    "        :return: The action to take.\n",
    "        \"\"\"\n",
    "        # Convert the observation to a tensor\n",
    "        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(self.agent.device)\n",
    "        # Get the action from the agent\n",
    "        with torch.no_grad():\n",
    "            # The act method now expects the observation tensor directly\n",
    "            action, _, _ = self.agent.act(observation_tensor, timestep=0, timesteps=1)\n",
    "        # Scale the action\n",
    "        action = action.squeeze(0).cpu().numpy() * self.action_scale\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a66148",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m controller \u001b[38;5;241m=\u001b[39m \u001b[43mNetworkController\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m observation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m      4\u001b[0m action \u001b[38;5;241m=\u001b[39m controller\u001b[38;5;241m.\u001b[39mget_action(observation)\n",
      "Cell \u001b[0;32mIn[14], line 58\u001b[0m, in \u001b[0;36mNetworkController.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m policy \u001b[38;5;241m=\u001b[39m PolicyNetwork(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, clip_actions, clip_log_std,\n\u001b[1;32m     55\u001b[0m                        min_log_std, max_log_std, initial_log_std, device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Instantiate the PPO agent with the manually created policy\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Load the pre-trained weights\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mload(network_path)\n",
      "File \u001b[0;32m~/isaacsim-env/lib/python3.10/site-packages/skrl/agents/torch/ppo/ppo.py:188\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, models, memory, observation_space, action_space, device, cfg)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# set up preprocessors\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_preprocessor:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_preprocessor_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_modules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_preprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_preprocessor\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "controller = NetworkController()\n",
    "\n",
    "observation = torch.zeros(12)\n",
    "action = controller.get_action(observation)\n",
    "print(\"Action:\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1974a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaacsim-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
